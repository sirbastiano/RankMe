{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29007819",
   "metadata": {},
   "source": [
    "# RankMe: Getting Started Guide\n",
    "\n",
    "This notebook demonstrates how to use the RankMe library to analyze and benchmark neural network models. We'll walk through various metrics including network analysis, feature learning assessment, and performance evaluation.\n",
    "\n",
    "## Overview\n",
    "RankMe provides comprehensive tools for:\n",
    "- Network architecture analysis (parameters, size, FLOPs)\n",
    "- Feature learning quality assessment using the RankMe score\n",
    "- Standard ML metrics (accuracy, F1, IoU, MSE, etc.)\n",
    "- Inference time benchmarking\n",
    "- Complete model evaluation suites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292bf9d3",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "First, let's create a sample model to analyze. We'll use ResNet18 as our example throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1299eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "model = torchvision.models.resnet18(weights=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949ad71",
   "metadata": {},
   "source": [
    "## Individual Network Metrics\n",
    "\n",
    "RankMe provides several network-level metrics to analyze your model's characteristics. Let's explore each one individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c107de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Network Metrics for ResNet18 ===\n",
      "Total parameters: 11,689,512\n",
      "Trainable parameters: 11,689,512\n",
      "Model size (parameters only): 44.59 MB\n",
      "Model size (with buffers): 44.63 MB\n",
      "Estimated FLOPs: 1,824,033,792\n",
      "Average precision bits per parameter: 32.0\n",
      "Parameter dtype summary:\n",
      "  torch.float32: 11,689,512 parameters\n",
      "Average inference time (CPU): 12.01 ms\n",
      "GPU inference time: Not available (no CUDA device)\n"
     ]
    }
   ],
   "source": [
    "# Import the network metrics\n",
    "from rankme.network import ParamCount, ModelSizeMB, Flops, PrecisionBits, InferenceTime\n",
    "\n",
    "# Initialize the metrics\n",
    "param_count = ParamCount()\n",
    "model_size = ModelSizeMB()\n",
    "flops = Flops()\n",
    "precision_bits = PrecisionBits()\n",
    "inference_time = InferenceTime()\n",
    "\n",
    "# Calculate various network metrics for the ResNet18 model\n",
    "print(\"=== Network Metrics for ResNet18 ===\")\n",
    "\n",
    "# 1. Parameter count\n",
    "total_params = param_count(model)\n",
    "trainable_params = param_count(model, include_non_trainable=False)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# 2. Model size in MB\n",
    "size_params_only = model_size(model, include_buffers=False)\n",
    "size_with_buffers = model_size(model, include_buffers=True)\n",
    "print(f\"Model size (parameters only): {size_params_only:.2f} MB\")\n",
    "print(f\"Model size (with buffers): {size_with_buffers:.2f} MB\")\n",
    "\n",
    "# 3. FLOPs estimation (for typical ImageNet input 224x224)\n",
    "input_size = (1, 3, 224, 224)  # Batch=1, Channels=3, Height=224, Width=224\n",
    "estimated_flops = flops(model, input_size=input_size)\n",
    "print(f\"Estimated FLOPs: {estimated_flops:,.0f}\")\n",
    "\n",
    "# 4. Precision bits (average across all parameters)\n",
    "avg_bits = precision_bits(model)\n",
    "print(f\"Average precision bits per parameter: {avg_bits:.1f}\")\n",
    "\n",
    "# Get dtype summary\n",
    "dtype_summary = precision_bits.get_dtype_summary(model)\n",
    "print(\"Parameter dtype summary:\")\n",
    "for dtype_name, count in dtype_summary.items():\n",
    "    print(f\"  {dtype_name}: {count:,} parameters\")\n",
    "\n",
    "# 5. Inference time measurement\n",
    "cpu_time, gpu_time = inference_time(\n",
    "    model, \n",
    "    input_size=(1, 3, 224, 224),  # ImageNet input size\n",
    "    runs=50,  # Number of timing runs\n",
    "    warmup=10  # Number of warmup runs\n",
    ")\n",
    "print(f\"Average inference time (CPU): {cpu_time*1000:.2f} ms\")\n",
    "if not torch.isnan(torch.tensor(gpu_time)):\n",
    "    print(f\"Average inference time (GPU): {gpu_time*1000:.2f} ms\")\n",
    "else:\n",
    "    print(\"GPU inference time: Not available (no CUDA device)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9ab71",
   "metadata": {},
   "source": [
    "# Comprehensive Benchmark Suite\n",
    "\n",
    "The `ModelBenchmark` class provides a unified interface to run all available metrics on your model. This is the recommended approach for complete model evaluation.\n",
    "\n",
    "## What the Benchmark Includes:\n",
    "- **Network Metrics**: Parameter count, model size, FLOPs, precision analysis, inference timing\n",
    "- **Feature Learning**: RankMe score to assess representation quality\n",
    "- **Classification Metrics**: Accuracy, F1-score, precision, recall, IoU\n",
    "- **Regression Metrics**: MSE, MAE, RMSE, R¬≤ score\n",
    "\n",
    "Let's set up our model and run the full benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7ff47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.resnet18(weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f0d7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running comprehensive benchmark on ResNet18...\n",
      "üöÄ Starting Full Model Benchmark Suite\n",
      "==================================================\n",
      "üîç Computing Network Metrics...\n",
      "üìä Computing Feature Learning Metrics...\n",
      "üéØ Computing Classification Metrics...\n",
      "üìà Computing Regression Metrics...\n",
      "\n",
      "============================================================\n",
      "üìä MODEL BENCHMARK RESULTS\n",
      "============================================================\n",
      "\n",
      "üèóÔ∏è  Model: ResNet\n",
      "üìê Input Size: (1, 3, 224, 224)\n",
      "\n",
      "üîç NETWORK METRICS\n",
      "------------------------------\n",
      "Total Parameters: 11,689,512\n",
      "Trainable Parameters: 11,689,512\n",
      "Model Size: 44.63 MB\n",
      "FLOPs: 1,824,033,792\n",
      "Avg Precision: 32.0 bits\n",
      "CPU Inference: 11.71 ms\n",
      "\n",
      "üìä FEATURE LEARNING METRICS\n",
      "------------------------------\n",
      "RankMe Score: 0.5543\n",
      "RankMe (Centered): 0.5492\n",
      "\n",
      "üéØ CLASSIFICATION METRICS\n",
      "------------------------------\n",
      "Accuracy: 0.1250\n",
      "F1 Score: 0.1141\n",
      "Precision: 0.1167\n",
      "Recall: 0.1117\n",
      "Iou: 0.0611\n",
      "\n",
      "üìà REGRESSION METRICS\n",
      "------------------------------\n",
      "MSE: 1.6839\n",
      "MAE: 1.0945\n",
      "RMSE: 1.2976\n",
      "R2_SCORE: -2.2361\n",
      "\n",
      "============================================================\n",
      "\n",
      "Quick Summary:\n",
      "Parameters: 11,689,512\n",
      "Model Size: 44.63 MB\n",
      "RankMe Score: 0.5543\n",
      "Accuracy: 0.1250\n",
      "R¬≤ Score: -2.2361\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict, Any, Tuple, Optional, Union\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "\n",
    "# Import all available metrics\n",
    "from rankme import RankMe, Accuracy, F1Score, IoU, Precision, Recall, MSE, MAE, R2Score, RMSE\n",
    "from rankme.network import ParamCount, ModelSizeMB, Flops, PrecisionBits, InferenceTime\n",
    "from rankme.suite import ModelBenchmark\n",
    "\n",
    "# Example usage with the ResNet18 model\n",
    "benchmark = ModelBenchmark()\n",
    "\n",
    "# Generate some sample data for demonstration\n",
    "batch_size, num_classes = 32, 10\n",
    "\n",
    "# Sample embeddings (e.g., from model's feature extractor)\n",
    "sample_embeddings = torch.randn(batch_size, 512)  # 512-dim features\n",
    "\n",
    "# Sample classification data\n",
    "y_true_class = torch.randint(0, num_classes, (batch_size,))\n",
    "y_pred_class = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# Sample regression data  \n",
    "y_true_reg = torch.randn(batch_size)\n",
    "y_pred_reg = torch.randn(batch_size)\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "print(\"Running comprehensive benchmark on ResNet18...\")\n",
    "results = benchmark.full_benchmark(\n",
    "    model=model,\n",
    "    embeddings=sample_embeddings,\n",
    "    y_pred_class=y_pred_class,\n",
    "    y_true_class=y_true_class, \n",
    "    y_pred_reg=y_pred_reg,\n",
    "    y_true_reg=y_true_reg,\n",
    "    num_classes=num_classes,\n",
    "    input_size=(1, 3, 224, 224),\n",
    "    runs=20,  # For inference timing\n",
    "    warmup=5\n",
    ")\n",
    "\n",
    "# Pretty print all results\n",
    "benchmark.print_results(results)\n",
    "\n",
    "# You can also access individual results programmatically\n",
    "print(f\"\\nQuick Summary:\")\n",
    "print(f\"Parameters: {results['network']['total_parameters']:,}\")\n",
    "print(f\"Model Size: {results['network']['size_total_mb']:.2f} MB\") \n",
    "if 'feature_learning' in results:\n",
    "    print(f\"RankMe Score: {results['feature_learning']['rankme_score']:.4f}\")\n",
    "if 'classification' in results:\n",
    "    print(f\"Accuracy: {results['classification']['accuracy']:.4f}\")\n",
    "if 'regression' in results:\n",
    "    print(f\"R¬≤ Score: {results['regression']['r2_score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rankme-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
