"""Tests for classification metrics."""

import pytest
import torch
import numpy as np

from rankme.classification import Accuracy, Precision, Recall, F1Score, IoU
from tests.conftest import assert_tensor_close, assert_in_range


class TestAccuracy:
    """Test cases for Accuracy metric."""
    
    def test_binary_accuracy(self, binary_classification_data):
        """Test binary accuracy computation."""
        y_true, y_pred_logits, y_pred_probs = binary_classification_data
        
        acc = Accuracy(task='binary')
        
        # Test with logits
        accuracy = acc(y_pred_logits, y_true)
        assert_in_range(accuracy, 0.0, 1.0)
        
        # Test with probabilities
        accuracy_probs = acc(y_pred_probs, y_true)
        assert_in_range(accuracy_probs, 0.0, 1.0)
        
    def test_multiclass_accuracy(self, multiclass_classification_data):
        """Test multiclass accuracy computation."""
        y_true, y_pred_logits, y_pred_classes, num_classes = multiclass_classification_data
        
        acc = Accuracy(task='multiclass', num_classes=num_classes)
        
        # Test with class predictions
        accuracy = acc(y_pred_classes, y_true)
        assert_in_range(accuracy, 0.0, 1.0)
        
        # Test with logits
        accuracy_logits = acc(y_pred_logits, y_true)
        assert_in_range(accuracy_logits, 0.0, 1.0)
        
    def test_top_k_accuracy(self, multiclass_classification_data):
        """Test top-k accuracy computation."""
        y_true, y_pred_logits, _, num_classes = multiclass_classification_data
        
        acc_top1 = Accuracy(task='multiclass', num_classes=num_classes, top_k=1)
        acc_top3 = Accuracy(task='multiclass', num_classes=num_classes, top_k=3)
        
        acc1 = acc_top1(y_pred_logits, y_true)
        acc3 = acc_top3(y_pred_logits, y_true)
        
        # Top-3 should be >= top-1
        assert acc3 >= acc1
        
    def test_multilabel_accuracy(self, multilabel_classification_data):
        """Test multilabel accuracy computation."""
        y_true, y_pred_logits, y_pred_probs, num_classes = multilabel_classification_data
        
        acc = Accuracy(task='multilabel', num_classes=num_classes)
        
        # Test with probabilities
        accuracy = acc(y_pred_probs, y_true)
        assert_in_range(accuracy, 0.0, 1.0)
        
    def test_perfect_accuracy(self):
        """Test with perfect predictions."""
        y_true = torch.tensor([0, 1, 2, 1, 0])
        y_pred = y_true.clone()
        
        acc = Accuracy(task='multiclass', num_classes=3)
        accuracy = acc(y_pred, y_true)
        
        assert_tensor_close(accuracy, 1.0)
        
    def test_zero_accuracy(self):
        """Test with completely wrong predictions."""
        y_true = torch.tensor([0, 0, 0, 0, 0])
        y_pred = torch.tensor([1, 1, 1, 1, 1])
        
        acc = Accuracy(task='multiclass', num_classes=2)
        accuracy = acc(y_pred, y_true)
        
        assert_tensor_close(accuracy, 0.0)


class TestPrecision:
    """Test cases for Precision metric."""
    
    def test_binary_precision(self):
        """Test binary precision computation."""
        y_true = torch.tensor([1, 1, 0, 0, 1])
        y_pred = torch.tensor([1, 0, 0, 0, 1])
        
        prec = Precision(task='binary')
        precision = prec(y_pred, y_true)
        
        # TP = 2, FP = 0, so precision = 2/2 = 1.0
        assert_tensor_close(precision, 1.0)
        
    def test_multiclass_precision(self, multiclass_classification_data):
        """Test multiclass precision computation."""
        y_true, _, y_pred_classes, num_classes = multiclass_classification_data
        
        prec_macro = Precision(task='multiclass', num_classes=num_classes, average='macro')
        prec_micro = Precision(task='multiclass', num_classes=num_classes, average='micro')
        prec_none = Precision(task='multiclass', num_classes=num_classes, average='none')
        
        precision_macro = prec_macro(y_pred_classes, y_true)
        precision_micro = prec_micro(y_pred_classes, y_true)
        precision_none = prec_none(y_pred_classes, y_true)
        
        assert_in_range(precision_macro, 0.0, 1.0)
        assert_in_range(precision_micro, 0.0, 1.0)
        assert precision_none.shape == (num_classes,)
        assert_in_range(precision_none, 0.0, 1.0)
        
    def test_precision_zero_division(self):
        """Test precision with zero division case."""
        y_true = torch.tensor([0, 0, 0, 0])
        y_pred = torch.tensor([0, 0, 0, 0])
        
        prec = Precision(task='binary', zero_division=0.5)
        precision = prec(y_pred, y_true)
        
        # No positive predictions, should return zero_division value
        assert_tensor_close(precision, 0.5)


class TestRecall:
    """Test cases for Recall metric."""
    
    def test_binary_recall(self):
        """Test binary recall computation."""
        y_true = torch.tensor([1, 1, 0, 0, 1])
        y_pred = torch.tensor([1, 0, 0, 0, 1])
        
        rec = Recall(task='binary')
        recall = rec(y_pred, y_true)
        
        # TP = 2, FN = 1, so recall = 2/3
        assert_tensor_close(recall, 2.0/3.0, atol=1e-5)
        
    def test_multiclass_recall(self, multiclass_classification_data):
        """Test multiclass recall computation."""
        y_true, _, y_pred_classes, num_classes = multiclass_classification_data
        
        rec_macro = Recall(task='multiclass', num_classes=num_classes, average='macro')
        rec_micro = Recall(task='multiclass', num_classes=num_classes, average='micro')
        rec_none = Recall(task='multiclass', num_classes=num_classes, average='none')
        
        recall_macro = rec_macro(y_pred_classes, y_true)
        recall_micro = rec_micro(y_pred_classes, y_true)
        recall_none = rec_none(y_pred_classes, y_true)
        
        assert_in_range(recall_macro, 0.0, 1.0)
        assert_in_range(recall_micro, 0.0, 1.0)
        assert recall_none.shape == (num_classes,)
        assert_in_range(recall_none, 0.0, 1.0)
        
    def test_perfect_recall(self):
        """Test with perfect recall."""
        y_true = torch.tensor([1, 1, 1, 1])
        y_pred = torch.tensor([1, 1, 1, 1])
        
        rec = Recall(task='binary')
        recall = rec(y_pred, y_true)
        
        assert_tensor_close(recall, 1.0)


class TestF1Score:
    """Test cases for F1Score metric."""
    
    def test_binary_f1(self):
        """Test binary F1-score computation."""
        y_true = torch.tensor([1, 1, 0, 0, 1])
        y_pred = torch.tensor([1, 0, 0, 0, 1])
        
        f1 = F1Score(task='binary')
        f1_score = f1(y_pred, y_true)
        
        # Precision = 1.0, Recall = 2/3, F1 = 2*1*(2/3)/(1+2/3) = 4/5
        expected_f1 = 2 * 1.0 * (2.0/3.0) / (1.0 + 2.0/3.0)
        assert_tensor_close(f1_score, expected_f1, atol=1e-5)
        
    def test_multiclass_f1(self, multiclass_classification_data):
        """Test multiclass F1-score computation."""
        y_true, _, y_pred_classes, num_classes = multiclass_classification_data
        
        f1_macro = F1Score(task='multiclass', num_classes=num_classes, average='macro')
        f1_micro = F1Score(task='multiclass', num_classes=num_classes, average='micro')
        
        f1_macro_score = f1_macro(y_pred_classes, y_true)
        f1_micro_score = f1_micro(y_pred_classes, y_true)
        
        assert_in_range(f1_macro_score, 0.0, 1.0)
        assert_in_range(f1_micro_score, 0.0, 1.0)
        
    def test_f1_perfect_score(self):
        """Test F1-score with perfect predictions."""
        y_true = torch.tensor([0, 1, 2, 1, 0])
        y_pred = y_true.clone()
        
        f1 = F1Score(task='multiclass', num_classes=3, average='macro')
        f1_score = f1(y_pred, y_true)
        
        assert_tensor_close(f1_score, 1.0)


class TestIoU:
    """Test cases for IoU metric."""
    
    def test_binary_iou(self):
        """Test binary IoU computation."""
        y_true = torch.tensor([1, 1, 0, 0, 1])
        y_pred = torch.tensor([1, 0, 0, 0, 1])
        
        iou = IoU(task='binary')
        iou_score = iou(y_pred, y_true)
        
        # Intersection = 2, Union = 3, IoU = 2/3
        assert_tensor_close(iou_score, 2.0/3.0, atol=1e-5)
        
    def test_multiclass_iou(self, multiclass_classification_data):
        """Test multiclass IoU computation."""
        y_true, _, y_pred_classes, num_classes = multiclass_classification_data
        
        iou_macro = IoU(task='multiclass', num_classes=num_classes, average='macro')
        iou_micro = IoU(task='multiclass', num_classes=num_classes, average='micro')
        iou_none = IoU(task='multiclass', num_classes=num_classes, average='none')
        
        iou_macro_score = iou_macro(y_pred_classes, y_true)
        iou_micro_score = iou_micro(y_pred_classes, y_true)
        iou_none_score = iou_none(y_pred_classes, y_true)
        
        assert_in_range(iou_macro_score, 0.0, 1.0)
        assert_in_range(iou_micro_score, 0.0, 1.0)
        assert iou_none_score.shape == (num_classes,)
        assert_in_range(iou_none_score, 0.0, 1.0)
        
    def test_perfect_iou(self):
        """Test IoU with perfect predictions."""
        y_true = torch.tensor([0, 1, 2, 1, 0])
        y_pred = y_true.clone()
        
        iou = IoU(task='multiclass', num_classes=3, average='macro')
        iou_score = iou(y_pred, y_true)
        
        assert_tensor_close(iou_score, 1.0)
        
    def test_iou_ignore_index(self):
        """Test IoU with ignored class."""
        y_true = torch.tensor([0, 1, 2, 255, 0])  # 255 is ignore class
        y_pred = torch.tensor([0, 1, 1, 255, 0])
        
        iou = IoU(task='multiclass', num_classes=3, ignore_index=255, average='macro')
        iou_score = iou(y_pred, y_true)
        
        # Should ignore the 255 class
        assert torch.isfinite(iou_score)
        assert_in_range(iou_score, 0.0, 1.0)


class TestClassificationIntegration:
    """Integration tests for classification metrics."""
    
    def test_metric_consistency(self, multiclass_classification_data):
        """Test consistency across different metrics."""
        y_true, _, y_pred_classes, num_classes = multiclass_classification_data
        
        acc = Accuracy(task='multiclass', num_classes=num_classes)
        prec = Precision(task='multiclass', num_classes=num_classes, average='micro')
        rec = Recall(task='multiclass', num_classes=num_classes, average='micro')
        f1 = F1Score(task='multiclass', num_classes=num_classes, average='micro')
        
        accuracy = acc(y_pred_classes, y_true)
        precision = prec(y_pred_classes, y_true)
        recall = rec(y_pred_classes, y_true)
        f1_score = f1(y_pred_classes, y_true)
        
        # For multiclass with micro averaging, precision = recall = accuracy
        assert_tensor_close(accuracy, precision, rtol=1e-5)
        assert_tensor_close(accuracy, recall, rtol=1e-5)
        assert_tensor_close(f1_score, accuracy, rtol=1e-5)
        
    def test_threshold_consistency(self, binary_classification_data):
        """Test that different thresholds give different results."""
        y_true, y_pred_logits, _ = binary_classification_data
        
        acc_05 = Accuracy(task='binary', threshold=0.5)
        acc_07 = Accuracy(task='binary', threshold=0.7)
        
        accuracy_05 = acc_05(y_pred_logits, y_true)
        accuracy_07 = acc_07(y_pred_logits, y_true)
        
        # Different thresholds should generally give different results
        # (unless data is very skewed)
        assert torch.isfinite(accuracy_05)
        assert torch.isfinite(accuracy_07)
        
    def test_edge_cases(self):
        """Test edge cases for classification metrics."""
        # Empty predictions
        y_true_empty = torch.tensor([], dtype=torch.long)
        y_pred_empty = torch.tensor([], dtype=torch.long)
        
        acc = Accuracy(task='multiclass', num_classes=2)
        
        # Should handle empty tensors gracefully
        accuracy_empty = acc(y_pred_empty, y_true_empty)
        assert torch.isfinite(accuracy_empty)
        
        # Single sample
        y_true_single = torch.tensor([1])
        y_pred_single = torch.tensor([1])
        
        accuracy_single = acc(y_pred_single, y_true_single)
        assert_tensor_close(accuracy_single, 1.0)
        
    def test_device_consistency(self, multiclass_classification_data, device):
        """Test device consistency for classification metrics."""
        y_true, _, y_pred_classes, num_classes = multiclass_classification_data
        
        acc = Accuracy(task='multiclass', num_classes=num_classes)
        
        # CPU computation
        accuracy_cpu = acc(y_pred_classes, y_true)
        
        # GPU computation (if available)
        if device.type == 'cuda':
            y_true_gpu = y_true.to(device)
            y_pred_gpu = y_pred_classes.to(device)
            acc_gpu = acc.to(device)
            
            accuracy_gpu = acc_gpu(y_pred_gpu, y_true_gpu)
            
            # Should be identical
            assert_tensor_close(accuracy_cpu, accuracy_gpu.cpu())